{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP AS2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpvt/zA19+/1iOt+yjp0sT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharace/COMP8730_Assign02_hirpara3/blob/main/NLP_AS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlqcHEl2eVBh",
        "outputId": "78c43bbc-2697-43d8-81af-11a20468498b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.7/dist-packages (0.5)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrec_eval\n",
        "import pytrec_eval\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.util import ngrams \n",
        "from nltk import bigrams, trigrams\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "import collections\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from operator import itemgetter\n",
        "import itertools, functools, operator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = '/content/sample_data/sentences.txt'\n",
        "\n",
        "with open(FILE_PATH, mode='r', encoding='utf8') as in_f:\n",
        "      lines = in_f.readlines()\n",
        "\n",
        "listOfSentences = [i.split(\"*\") for i in lines]\n",
        "listOfSentences = [i[0].split() for i in listOfSentences]\n",
        "\n",
        "listOfIncompleteSentence = [i[2:4] for i in listOfSentences]\n",
        "listOfCorrectWords = [i[1] for i in listOfSentences]\n",
        "\n",
        "print(listOfIncompleteSentence)\n",
        "print(listOfCorrectWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV-An1jxfpyx",
        "outputId": "95af743b-ccb3-48af-bdec-274fc3bfd720"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'felt'], ['at'], ['when', 'the'], ['in', 'the'], ['I', 'thought'], ['everything'], ['when', 'I'], ['and', 'saw'], ['and', 'saw'], ['I', 'was'], ['I'], ['through', 'the'], ['the', 'hunters'], ['they', 'kill'], ['make', 'a'], ['to', 'tidy'], ['the', 'wind'], ['Mr', 'J.'], ['garden', 'full'], ['talk', 'to'], ['they', 'throw'], ['an'], ['after', 'the'], ['the', 'birds'], ['making', 'any'], ['bring', 'it'], ['this'], ['a', 'man'], ['an'], ['they'], ['a'], ['this'], ['they', 'make'], ['the', 'animals'], ['the'], ['they', 'throw'], ['they'], ['three'], ['it', 'was'], ['I', 'saw'], ['could', 'do'], ['blood', 'was'], ['you', 'have'], ['you', 'can'], [], ['out', 'from'], ['for', 'controlling'], ['friendly', 'with'], ['the'], ['to', 'make'], ['hit', 'him'], ['they'], ['transport', 'is'], ['they', 'have'], ['a', 'plastic'], ['the'], ['a'], ['to'], ['on', 'and'], ['if', 'you'], ['a'], ['all', 'of'], ['went', 'up'], ['with'], ['so', 'I'], ['I', 'made'], ['I', 'was'], [], ['one', 'of'], ['found', 'some'], ['I', 'was'], ['speaking', 'to'], ['to'], ['lit', 'a'], ['a', 'man'], ['something'], [], ['the', 'inspector,'], ['a', 'light'], ['I', 'would'], [], ['Ian'], ['got', 'a'], [], [\"I'm\"], ['a'], ['I'], ['he', 'just'], [], ['ate', 'some'], ['he', \"doesn't\"], ['a', 'bull'], ['and', 'ice-cream'], ['there', 'was'], ['then', 'a'], [], ['four', 'pen'], ['there', 'are'], ['on', 'the'], ['on', 'the'], ['in', 'the'], ['the', 'light'], ['from', 'the'], ['to', 'the'], ['in', 'a'], ['a'], ['the'], ['fried', 'rice'], ['carry', 'money'], ['they', 'are'], ['the', 'things'], ['differences', 'between'], ['by', 'logical'], ['a', 'white'], [\"don't\", 'make'], ['to'], ['when', 'a'], ['the'], ['the', 'dustpin'], ['the', 'roof'], ['have', 'to'], ['they'], ['he'], ['the', 'most'], ['of', 'the'], ['the', 'man'], ['put', 'more'], ['he'], ['heard', 'a'], ['he'], ['he'], [\"he'd\"], [\"he'd\"], [\"he'd\"], ['sent', 'them'], [], [], ['put', 'on'], ['what', 'had'], [\"he'd\"], [], ['what', 'had'], ['in'], [], [], [\"it's\", 'a'], ['they', \"aren't\"], ['to'], ['an'], ['the'], [], ['it'], ['it', 'is'], ['is'], ['there', 'are'], ['it', 'was'], ['to'], ['my'], ['the'], [], [], ['the', 'most'], ['gamblers', 'play'], ['the'], [], ['it', 'is'], ['let', 'us'], [], ['I'], ['as', 'often'], ['flowers,'], ['to'], ['my'], ['surprised', 'and'], ['I'], ['when', 'it'], ['she', 'asked'], ['an'], ['be', 'able'], ['their'], ['to', 'think'], ['to'], ['it', 'was'], ['I', 'never'], [\"I'd\"], ['he'], ['to'], [], ['to'], ['I', \"couldn't\"], ['a', 'street'], ['a', 'movie'], [], [], ['a'], ['meet', 'in'], ['security'], ['in']]\n",
            "['strange', 'break', 'break', 'winter', 'ghost', 'except', 'stepped', 'strange', 'coloured', 'escalator', 'noticed', 'fence', 'kill', 'arrow', 'deep', 'garden', 'blew', 'angry', 'leaves', 'manager', 'arrow', 'ancient', 'deer', 'flew', 'noise', 'straight', 'means', 'waits', 'arrow', 'dig', 'piece', 'trap', 'noise', 'scared', 'rabbits', 'stones', 'chasing', 'hundred', 'colourful', 'clowns', 'easily', 'coming', 'wandered', 'control', 'chatting', 'wardrobe', 'temperature', 'neighbours', 'princess', 'arrangements', 'hammer', 'might', 'different', 'clothes', 'sharpener', 'chinese', 'bottle', 'hear', 'off', 'thirsty', 'match', 'sudden', 'curtain', 'bubbles', 'bath', 'clothes', 'scared', 'through', 'exciting', 'crumbs', 'careful', 'ghost', 'make', 'candle', 'lady', 'bad', 'thunder', 'sergeant', 'noise', 'rub', 'crush', 'tripped', 'tissue', 'what', 'writing', 'straight', 'thought', 'wanted', 'bouncing', 'fruits', 'notice', 'charging', 'stall', 'giraffe', 'crocodile', 'iron', 'knives', 'chimneys', 'blackboard', 'globe', 'cupboard', 'switch', 'thinnest', 'fattest', 'field', 'cherry', 'beginning', 'noodles', 'with', 'believe', 'believe', 'knowledge', 'speech', 'colour', 'break', 'find', 'couple', 'dustbin', 'emptied', 'repaired', 'emptied', 'were', 'believes', 'famous', 'century', 'tried', 'petrol', 'tried', 'strange', 'tried', 'hit', 'forgotten', 'set', 'shot', 'prison', 'crying', 'last', 'weight', 'happened', 'saved', 'become', 'happened', 'holidays', 'photographs', 'which', 'wolf', 'discovered', 'photograph', 'unborn', 'mysterious', 'choosing', 'represents', 'ridiculous', 'scientifically', 'advertisements', 'imagination', 'ban', 'happiness', 'pleasant', 'dialling', 'actually', 'tiring', 'poker', 'existence', 'physical', 'doubt', 'analyse', 'developing', 'personally', 'breathe', 'perfumes', 'interpret', 'stomach', 'paralysed', 'believe', 'occurred', 'whether', 'extremely', 'choose', 'opinion', 'what', 'continue', 'quite', 'thought', 'heard', 'obviously', 'praise', 'screaming', 'absorb', 'staring', 'cafe', 'shown', 'companies', 'there', 'tendency', 'snack-bar', 'control', 'contact']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_news=brown.words(categories=[\"news\"])\n",
        "print(words_news)\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "print(model)\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L4uCCz_euAG",
        "outputId": "2dec6996-3603-487e-83a8-1049622783bd"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "defaultdict(<function <lambda> at 0x7f5d2bfa6950>, {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dict1 = dict(model[\"make\",\"a\"])\n",
        "# print(dict1)\n",
        "\n",
        "dict1 = {'the': 0.2857142857142857, 'a': 0.14285714285714285, 'full': 0.07142857142857142, 'failing': 0.07142857142857142, 'as': 0.07142857142857142, 'to': 0.07142857142857142, 'overwhelming': 0.07142857142857142, 'that': 0.07142857142857142, 'incorporation': 0.07142857142857142, 'often': 0.07142857142857142}\n",
        "\n",
        "dict1 = {key: rank for rank, key in enumerate(sorted(dict1, key=dict1.get, reverse=True), 1)}\n",
        "\n",
        "lista = [x for x in dict1.keys()]\n",
        "print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ORT66VZfa9J",
        "outputId": "97f65812-f61a-4b98-b404-633adc03fbb4"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'a', 'full', 'failing', 'as', 'to', 'overwhelming', 'that', 'incorporation', 'often']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_ngrams = []\n",
        "for i in ([1,2,3,5,10]):\n",
        "    ng = ngrams(brown.words(),i)\n",
        "    list_of_ngrams.append(ng)"
      ],
      "metadata": {
        "id": "L677AQ3vgfkN"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngram model\n",
        "model_ngram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in model_ngram:\n",
        "    for w1 in ngrams(model_ngram,1, pad_right=True, pad_left=True):\n",
        "        model_ngram[(w1, w2)][w3] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_ngram:\n",
        "    total_count = float(sum(model_ngram[w1_w2].values()))\n",
        "    for w3 in model_ngram[w1_w2]:\n",
        "        model_ngram[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "id": "AXZDQ_XuhAn-"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram model\n",
        "\n",
        "model_bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model_bigram[(w1)][w3] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_bigram:\n",
        "    total_count = float(sum(model_bigram[w1_w2].values()))\n",
        "    for w3 in model_bigram[w1_w2]:\n",
        "        model_bigram[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "id": "CxvlSgXGkgaf"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model_trigram[(w1, w2)][w3] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_trigram:\n",
        "    total_count = float(sum(model_trigram[w1_w2].values()))\n",
        "    for w3 in model_trigram[w1_w2]:\n",
        "        model_trigram[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "fL5Ow4oC1rMm"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fivegram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3, w4,w5 in ngrams(sentence, 5, pad_right=True, pad_left=True):\n",
        "        model_fivegram[(w1,w2, w3, w4)][w5] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_fivegram:\n",
        "    total_count = float(sum(model_fivegram[w1_w2].values()))\n",
        "    for w3 in model_fivegram[w1_w2]:\n",
        "        model_fivegram[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "HyXyETsnPv7I"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tengram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3, w4,w5, w6, w7, w8,w9, w10 in ngrams(sentence, 10, pad_right=True, pad_left=True):\n",
        "        model_tengram[(w1,w2, w3, w4,w5, w6, w7, w8,w9,)][w10] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_tengram:\n",
        "    total_count = float(sum(model_tengram[w1_w2].values()))\n",
        "    for w10 in model_tengram[w1_w2]:\n",
        "        model_tengram[w1_w2][w10] /= total_count"
      ],
      "metadata": {
        "id": "TYNgEwGeSKGh"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(listOfSentences)\n",
        "print(listOfSentences)\n",
        "\n",
        "listOfDicts = []\n",
        "def funcA(sentences, model):\n",
        "    for sentence in sentences:\n",
        "        l = []\n",
        "        dict2 = {}\n",
        "        text = sentence\n",
        "        text_length_initial = len(text)\n",
        "        res = dict(sorted(model[tuple(text[-text_length_initial:])].items(), key = itemgetter(1), reverse = True)[:10])\n",
        "        lista = [x for x in res.keys()]\n",
        "        listOfDicts.append(lista) \n",
        "    return(listOfDicts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9_Hrl1XSjh7",
        "outputId": "5889041e-ab37-4f59-eb9b-db80ec58ec07"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['strang', 'strange', 'I', 'felt', 'very'], ['brake', 'break', 'at'], ['brack', 'break', 'when', 'the'], ['weanter', 'winter', 'in', 'the'], ['gost', 'ghost', 'I', 'thought', 'it', 'was', 'a'], ['expect', 'except', 'everything'], ['steped', 'stepped', 'when', 'I', 'first'], ['streagh', 'strange', 'and', 'saw'], ['colow', 'coloured', 'and', 'saw', 'streagh'], ['exclation', 'escalator', 'I', 'was', 'on', 'an'], ['noicey', 'noticed', 'I'], ['fance', 'fence', 'through', 'the'], ['kille', 'kill', 'the', 'hunters'], ['nerrow', 'arrow', 'they', 'kill', 'birds', 'with', 'their'], ['depe', 'deep', 'make', 'a'], ['gardon', 'garden', 'to', 'tidy', 'up', 'his'], ['belu', 'blew', 'the', 'wind'], ['angray', 'angry', 'Mr', 'J.', 'was', 'very'], ['leavs', 'leaves', 'garden', 'full', 'of'], ['manger', 'manager', 'talk', 'to', 'the'], ['aero', 'arrow', 'they', 'throw', 'a'], ['ansion', 'ancient', 'an'], ['dear', 'deer', 'after', 'the'], ['flu', 'flew', 'the', 'birds'], ['noice', 'noise', 'making', 'any'], ['stright', 'straight', 'bring', 'it'], ['menes', 'means', 'this'], ['waight', 'waits', 'a', 'man'], ['arow', 'arrow', 'an'], ['dick', 'dig', 'they'], ['pice', 'piece', 'a'], ['tarpp', 'trap', 'this'], ['nose', 'noise', 'they', 'make'], ['skeard', 'scared', 'the', 'animals', 'are'], ['ribbetes', 'rabbits', 'the'], ['stons', 'stones', 'they', 'throw'], ['chaching', 'chasing', 'they'], ['handred', 'hundred', 'three'], ['coulorful', 'colourful', 'it', 'was'], ['crowns', 'clowns', 'I', 'saw', 'many'], ['easly', 'easily', 'could', 'do', 'it'], ['comming', 'coming', 'blood', 'was'], ['wondered', 'wandered', 'you', 'have'], ['controle', 'control', 'you', 'can'], ['chating', 'chatting'], ['wardrope', 'wardrobe', 'out', 'from', 'the'], ['temerature', 'temperature', 'for', 'controlling'], ['neibours', 'neighbours', 'friendly', 'with', 'your'], ['prinsess', 'princess', 'the'], ['arrengiments', 'arrangements', 'to', 'make'], ['hamer', 'hammer', 'hit', 'him', 'with', 'a'], ['mite', 'might', 'they'], ['deferent', 'different', 'transport', 'is'], ['cloves', 'clothes', 'they', 'have', 'different'], ['sharpner', 'sharpener', 'a', 'plastic'], ['chinease', 'chinese', 'the'], ['bottal', 'bottle', 'a'], ['here', 'hear', 'to'], ['of', 'off', 'on', 'and'], ['thisty', 'thirsty', 'if', 'you', 'are'], ['march', 'match', 'a'], ['sarden', 'sudden', 'all', 'of', 'a'], ['certon', 'curtain', 'went', 'up', 'the'], ['babbles', 'bubbles', 'with'], ['barth', 'bath', 'so', 'I', 'had', 'a'], ['clouths', 'clothes', 'I', 'made'], ['skeard', 'scared', 'I', 'was'], ['throw', 'through'], ['exsiting', 'exciting', 'one', 'of', 'my'], ['crames', 'crumbs', 'found', 'some'], ['carful', 'careful', 'I', 'was', 'very'], ['gost', 'ghost', 'speaking', 'to', 'a'], ['mak', 'make', 'to'], ['candel', 'candle', 'lit', 'a'], ['laddy', 'lady', 'a', 'man', 'or', 'a'], ['bard', 'bad', 'something'], ['thander', 'thunder'], ['sargent', 'sergeant', 'the', 'inspector,', 'the'], ['nose', 'noise', 'a', 'light', 'and', 'a'], ['rube', 'rub', 'I', 'would'], ['cruch', 'crush'], ['triped', 'tripped', 'Ian'], ['tuisse', 'tissue', 'got', 'a'], ['want', 'what'], ['writting', 'writing', \"I'm\"], ['straigt', 'straight', 'a'], ['througt', 'thought', 'I'], ['whated', 'wanted', 'he', 'just'], ['bounsing', 'bouncing'], ['fruts', 'fruits', 'ate', 'some'], ['noteics', 'notice', 'he', \"doesn't\"], ['charcing', 'charging', 'a', 'bull'], ['stroe', 'stall', 'and', 'ice-cream'], ['girrife', 'giraffe', 'there', 'was', 'a'], ['circodile', 'crocodile', 'then', 'a'], ['iorn', 'iron'], ['kives', 'knives', 'four', 'pen'], ['chimmys', 'chimneys', 'there', 'are', 'four'], ['blackbroad', 'blackboard', 'on', 'the'], ['golb', 'globe', 'on', 'the'], ['cupbroad', 'cupboard', 'in', 'the'], ['swith', 'switch', 'the', 'light'], ['thinest', 'thinnest', 'from', 'the'], ['fatest', 'fattest', 'to', 'the'], ['feild', 'field', 'in', 'a'], ['cerry', 'cherry', 'a'], ['being', 'beginning', 'the'], ['nodles', 'noodles', 'fried', 'rice', 'and'], ['whith', 'with', 'carry', 'money'], ['belive', 'believe', 'they', 'are'], ['belive', 'believe', 'the', 'things', 'I'], ['knowledg', 'knowledge', 'differences', 'between', 'their'], ['speach', 'speech', 'by', 'logical'], ['coulor', 'colour', 'a', 'white'], ['beark', 'break', \"don't\", 'make', 'anything'], ['feind', 'find', 'to'], ['cuple', 'couple', 'when', 'a'], ['dustpin', 'dustbin', 'the'], ['emptyed', 'emptied', 'the', 'dustpin', 'is', 'being'], ['repared', 'repaired', 'the', 'roof', 'is', 'being'], ['emtied', 'emptied', 'have', 'to', 'be'], ['where', 'were', 'they'], ['belive', 'believes', 'he'], ['fameous', 'famous', 'the', 'most'], ['centery', 'century', 'of', 'the'], ['tired', 'tried', 'the', 'man', 'had'], ['petroil', 'petrol', 'put', 'more'], ['tryed', 'tried', 'he'], ['extrange', 'strange', 'heard', 'a'], ['tryed', 'tried', 'he'], ['hited', 'hit', 'he'], ['forgoten', 'forgotten', \"he'd\"], ['setted', 'set', \"he'd\"], ['shooted', 'shot', \"he'd\"], ['prision', 'prison', 'sent', 'them', 'to'], ['craying', 'crying'], ['las', 'last'], ['weigh', 'weight', 'put', 'on'], ['happend', 'happened', 'what', 'had'], ['safed', 'saved', \"he'd\"], ['becom', 'become'], ['happend', 'happened', 'what', 'had'], ['hollidays', 'holidays', 'in'], ['photographes', 'photographs'], ['wich', 'which'], ['wolfe', 'wolf', \"it's\", 'a'], ['discoverd', 'discovered', 'they', \"aren't\"], ['photographe', 'photograph', 'to'], ['unborne', 'unborn', 'an'], ['misterious', 'mysterious', 'the'], ['chosing', 'choosing'], ['rapresents', 'represents', 'it'], ['ridicolous', 'ridiculous', 'it', 'is'], ['scientificly', 'scientifically', 'is'], ['advertisments', 'advertisements', 'there', 'are'], ['immagination', 'imagination', 'it', 'was', 'my'], ['bann', 'ban', 'to'], ['happyness', 'happiness', 'my'], ['pleasent', 'pleasant', 'the'], ['dialing', 'dialling'], ['actualy', 'actually'], ['tireing', 'tiring', 'the', 'most'], ['pocker', 'poker', 'gamblers', 'play'], ['existance', 'existence', 'the'], ['phisical', 'physical'], ['doupt', 'doubt', 'it', 'is', 'without'], ['analize', 'analyse', 'let', 'us'], ['devlopping', 'developing'], ['personnaly', 'personally', 'I'], ['breath', 'breathe', 'as', 'often', 'as', 'I'], ['perfums', 'perfumes', 'flowers,'], ['interprete', 'interpret', 'to'], ['stomache', 'stomach', 'my'], ['paralised', 'paralysed', 'surprised', 'and'], ['belief', 'believe', 'I'], ['occured', 'occurred', 'when', 'it'], ['wether', 'whether', 'she', 'asked', 'us'], ['extremly', 'extremely', 'an'], ['chose', 'choose', 'be', 'able', 'to'], ['oppinion', 'opinion', 'their'], ['wat', 'what', 'to', 'think'], ['contenue', 'continue', 'to'], ['quiet', 'quite', 'it', 'was'], ['thout', 'thought', 'I', 'never'], ['heart', 'heard', \"I'd\"], ['obvisicly', 'obviously', 'he'], ['prays', 'praise', 'to'], ['screeming', 'screaming'], ['absorbe', 'absorb', 'to'], ['starring', 'staring', 'I', \"couldn't\", 'stop'], ['caffe', 'cafe', 'a', 'street'], ['showen', 'shown', 'a', 'movie', 'was'], ['companys', 'companies'], ['their', 'there'], ['tendancy', 'tendency', 'a'], ['snak-bar', 'snack-bar', 'meet', 'in', 'the'], ['controll', 'control', 'security'], ['condact', 'contact', 'in']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_predicted_model = []\n",
        "models = [model_ngram, model_bigram, model_trigram, model_fivegram, model_tengram]\n",
        "for model in models:\n",
        "    list_of_predicted_model.append(funcA(listOfSentences, model))\n",
        "\n",
        "len(list_of_predicted_model[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ki8_RxY31a3E",
        "outputId": "50ed027b-df55-4c03-ef6b-fd43378e2577"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3960"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qrel = listOfCorrectWords\n",
        "run = listOfDicts\n",
        "\n",
        "def s_at_k(correct_words, predicted_words, k=[1, 5, 10]):\n",
        "    count = []\n",
        "    for i in range(198):\n",
        "        words_candidate = predicted_words[i]\n",
        "        correct_word = correct_words[i]\n",
        "        for j in k:\n",
        "            if correct_word in words_candidate[:j]:\n",
        "                count.append(1)\n",
        "            else:\n",
        "                count.append(0)\n",
        "                pass\n",
        "    return(sum(count) / len(count))\n",
        "\n",
        "meansModelsK1 = []\n",
        "for i in range(5):\n",
        "    meansModelsK1.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[1]))\n",
        "print(meansModelsK1)"
      ],
      "metadata": {
        "id": "Lo-bAtjnUjhw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2371eb1d-d559-476f-954c-49c9db14eb29"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK1 = []\n",
        "for i in range(5):\n",
        "    meansModelsK1.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[1]))\n",
        "print(meansModelsK1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtU_jFAOU77S",
        "outputId": "e954f09a-0f98-43b7-decb-545e6ca85af6"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK5 = []\n",
        "for i in range(5):\n",
        "    meansModelsK5.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[5]))\n",
        "print(meansModelsK5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYU-PMAtXKVY",
        "outputId": "ceb39d3c-0f36-4e46-dc17-ac5650c4cd44"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK10 = []\n",
        "for i in range(5):\n",
        "    meansModelsK10.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[10]))\n",
        "print(meansModelsK10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-TskXNCXlJb",
        "outputId": "8ab52f4c-cab0-442c-d570-0c6786e2c15c"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in list_of_ngrams:\n",
        "    qrel = listOfCorrectWords \n",
        "    run = listOfDicts\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        qrel,\n",
        "        {\n",
        "            \"success_1\",\n",
        "            \"success_5\",\n",
        "            \"success_10\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    res = evaluator.evaluate(run)\n",
        "    \n",
        "    # printing scores \n",
        "    successes = dict()\n",
        "    for i in range(1, 11):\n",
        "        successes[f\"success_{i}\"] = []\n",
        "\n",
        "    for inc in res:\n",
        "        tmp = res[inc]\n",
        "        for i in range(1, 11):\n",
        "            successes[f\"success_@_{i}\"].append(tmp[f\"success_@_{i}\"])\n",
        "    \n",
        "    successes = dict()\n",
        "    for i in range(1, 11):\n",
        "        successes[f\"success_@_{i}\"] = []\n",
        "\n",
        "    for inc in res:\n",
        "        tmp = res[inc]\n",
        "        for i in range(1, 11):\n",
        "            successes[f\"success_@_{i}\"].append(tmp[f\"success_@_{i}\"])\n",
        "\n",
        "    import numpy as np\n",
        "    for i in range(1, 11):\n",
        "        print(f\"average s@top-{i}: \", np.array(successes[f\"success_@_{i}\"]).mean())  \n",
        "\n",
        "# Note: Error found in pytrec_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "hrHLMTlxXxzW",
        "outputId": "b84e0d2a-cd28-4b57-b53e-ee354fa04b2c"
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-216-7956e56f6f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;34m\"success_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;34m\"success_5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0;34m\"success_10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         },\n\u001b[1;32m     11\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytrec_eval/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_relevance, measures, relevance_level)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_nicknames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_relevance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelevance_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument query_relevance should be of type dictionary."
          ]
        }
      ]
    }
  ]
}