{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP AS2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOg/oe2mKd2Jky7194kKZXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dharace/COMP8730_Assign02_hirpara3/blob/main/NLP_AS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlqcHEl2eVBh",
        "outputId": "68e4035c-3ed6-43b6-dcaf-2a077a512705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytrec_eval in /usr/local/lib/python3.7/dist-packages (0.5)\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install pytrec_eval\n",
        "import pytrec_eval\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.util import ngrams \n",
        "from nltk import bigrams, trigrams\n",
        "\n",
        "nltk.download('brown')\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "\n",
        "import collections\n",
        "from nltk import word_tokenize\n",
        "\n",
        "from operator import itemgetter\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FILE_PATH = '/content/sample_data/sentences.txt'\n",
        "\n",
        "with open(FILE_PATH, mode='r', encoding='utf8') as in_f:\n",
        "      lines = in_f.readlines()\n",
        "\n",
        "listOfSentences = [i.split(\"*\") for i in lines]\n",
        "listOfSentences = [i[0].split() for i in listOfSentences]\n",
        "\n",
        "listOfIncompleteSentence = [i[2:4] for i in listOfSentences]\n",
        "listOfCorrectWords = [i[1] for i in listOfSentences]\n",
        "\n",
        "print(listOfIncompleteSentence)\n",
        "print(listOfCorrectWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV-An1jxfpyx",
        "outputId": "7d1f0f8c-6a6e-4969-f473-441511f3c624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'felt'], ['at'], ['when', 'the'], ['in', 'the'], ['I', 'thought'], ['everything'], ['when', 'I'], ['and', 'saw'], ['and', 'saw'], ['I', 'was'], ['I'], ['through', 'the'], ['the', 'hunters'], ['they', 'kill'], ['make', 'a'], ['to', 'tidy'], ['the', 'wind'], ['Mr', 'J.'], ['garden', 'full'], ['talk', 'to'], ['they', 'throw'], ['an'], ['after', 'the'], ['the', 'birds'], ['making', 'any'], ['bring', 'it'], ['this'], ['a', 'man'], ['an'], ['they'], ['a'], ['this'], ['they', 'make'], ['the', 'animals'], ['the'], ['they', 'throw'], ['they'], ['three'], ['it', 'was'], ['I', 'saw'], ['could', 'do'], ['blood', 'was'], ['you', 'have'], ['you', 'can'], [], ['out', 'from'], ['for', 'controlling'], ['friendly', 'with'], ['the'], ['to', 'make'], ['hit', 'him'], ['they'], ['transport', 'is'], ['they', 'have'], ['a', 'plastic'], ['the'], ['a'], ['to'], ['on', 'and'], ['if', 'you'], ['a'], ['all', 'of'], ['went', 'up'], ['with'], ['so', 'I'], ['I', 'made'], ['I', 'was'], [], ['one', 'of'], ['found', 'some'], ['I', 'was'], ['speaking', 'to'], ['to'], ['lit', 'a'], ['a', 'man'], ['something'], [], ['the', 'inspector,'], ['a', 'light'], ['I', 'would'], [], ['Ian'], ['got', 'a'], [], [\"I'm\"], ['a'], ['I'], ['he', 'just'], [], ['ate', 'some'], ['he', \"doesn't\"], ['a', 'bull'], ['and', 'ice-cream'], ['there', 'was'], ['then', 'a'], [], ['four', 'pen'], ['there', 'are'], ['on', 'the'], ['on', 'the'], ['in', 'the'], ['the', 'light'], ['from', 'the'], ['to', 'the'], ['in', 'a'], ['a'], ['the'], ['fried', 'rice'], ['carry', 'money'], ['they', 'are'], ['the', 'things'], ['differences', 'between'], ['by', 'logical'], ['a', 'white'], [\"don't\", 'make'], ['to'], ['when', 'a'], ['the'], ['the', 'dustpin'], ['the', 'roof'], ['have', 'to'], ['they'], ['he'], ['the', 'most'], ['of', 'the'], ['the', 'man'], ['put', 'more'], ['he'], ['heard', 'a'], ['he'], ['he'], [\"he'd\"], [\"he'd\"], [\"he'd\"], ['sent', 'them'], [], [], ['put', 'on'], ['what', 'had'], [\"he'd\"], [], ['what', 'had'], ['in'], [], [], [\"it's\", 'a'], ['they', \"aren't\"], ['to'], ['an'], ['the'], [], ['it'], ['it', 'is'], ['is'], ['there', 'are'], ['it', 'was'], ['to'], ['my'], ['the'], [], [], ['the', 'most'], ['gamblers', 'play'], ['the'], [], ['it', 'is'], ['let', 'us'], [], ['I'], ['as', 'often'], ['flowers,'], ['to'], ['my'], ['surprised', 'and'], ['I'], ['when', 'it'], ['she', 'asked'], ['an'], ['be', 'able'], ['their'], ['to', 'think'], ['to'], ['it', 'was'], ['I', 'never'], [\"I'd\"], ['he'], ['to'], [], ['to'], ['I', \"couldn't\"], ['a', 'street'], ['a', 'movie'], [], [], ['a'], ['meet', 'in'], ['security'], ['in']]\n",
            "['strange', 'break', 'break', 'winter', 'ghost', 'except', 'stepped', 'strange', 'coloured', 'escalator', 'noticed', 'fence', 'kill', 'arrow', 'deep', 'garden', 'blew', 'angry', 'leaves', 'manager', 'arrow', 'ancient', 'deer', 'flew', 'noise', 'straight', 'means', 'waits', 'arrow', 'dig', 'piece', 'trap', 'noise', 'scared', 'rabbits', 'stones', 'chasing', 'hundred', 'colourful', 'clowns', 'easily', 'coming', 'wandered', 'control', 'chatting', 'wardrobe', 'temperature', 'neighbours', 'princess', 'arrangements', 'hammer', 'might', 'different', 'clothes', 'sharpener', 'chinese', 'bottle', 'hear', 'off', 'thirsty', 'match', 'sudden', 'curtain', 'bubbles', 'bath', 'clothes', 'scared', 'through', 'exciting', 'crumbs', 'careful', 'ghost', 'make', 'candle', 'lady', 'bad', 'thunder', 'sergeant', 'noise', 'rub', 'crush', 'tripped', 'tissue', 'what', 'writing', 'straight', 'thought', 'wanted', 'bouncing', 'fruits', 'notice', 'charging', 'stall', 'giraffe', 'crocodile', 'iron', 'knives', 'chimneys', 'blackboard', 'globe', 'cupboard', 'switch', 'thinnest', 'fattest', 'field', 'cherry', 'beginning', 'noodles', 'with', 'believe', 'believe', 'knowledge', 'speech', 'colour', 'break', 'find', 'couple', 'dustbin', 'emptied', 'repaired', 'emptied', 'were', 'believes', 'famous', 'century', 'tried', 'petrol', 'tried', 'strange', 'tried', 'hit', 'forgotten', 'set', 'shot', 'prison', 'crying', 'last', 'weight', 'happened', 'saved', 'become', 'happened', 'holidays', 'photographs', 'which', 'wolf', 'discovered', 'photograph', 'unborn', 'mysterious', 'choosing', 'represents', 'ridiculous', 'scientifically', 'advertisements', 'imagination', 'ban', 'happiness', 'pleasant', 'dialling', 'actually', 'tiring', 'poker', 'existence', 'physical', 'doubt', 'analyse', 'developing', 'personally', 'breathe', 'perfumes', 'interpret', 'stomach', 'paralysed', 'believe', 'occurred', 'whether', 'extremely', 'choose', 'opinion', 'what', 'continue', 'quite', 'thought', 'heard', 'obviously', 'praise', 'screaming', 'absorb', 'staring', 'cafe', 'shown', 'companies', 'there', 'tendency', 'snack-bar', 'control', 'contact']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_news=brown.words(categories=[\"news\"])\n",
        "print(words_news)\n",
        "\n",
        "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "print(model)\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model[(w1, w2)][w3] += 1\n",
        "\n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L4uCCz_euAG",
        "outputId": "1a4294e8-a2d1-4f84-ac52-fad933daa5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
            "defaultdict(<function <lambda> at 0x7f5d648e9e60>, {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict1 = dict(model[\"make\",\"a\"])\n",
        "print(dict1)\n",
        "\n",
        "# dict1 = {'the': 0.2857142857142857, 'a': 0.14285714285714285, 'full': 0.07142857142857142, 'failing': 0.07142857142857142, 'as': 0.07142857142857142, 'to': 0.07142857142857142, 'overwhelming': 0.07142857142857142, 'that': 0.07142857142857142, 'incorporation': 0.07142857142857142, 'often': 0.07142857142857142}\n",
        "\n",
        "dict1 = {key: rank for rank, key in enumerate(sorted(dict1, key=dict1.get, reverse=True), 1)}\n",
        "\n",
        "lista = [x for x in dict1.keys()]\n",
        "print(lista)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ORT66VZfa9J",
        "outputId": "d6184164-46d0-4895-d7ef-24855660cd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'case': 0.009433962264150943, 'race': 0.009433962264150943, 'public': 0.009433962264150943, 'spectacle': 0.009433962264150943, 'valuable': 0.009433962264150943, 'good': 0.03773584905660377, 'trip': 0.009433962264150943, 'little': 0.03773584905660377, 'start': 0.018867924528301886, 'judgment': 0.009433962264150943, 'beginning': 0.009433962264150943, 'right': 0.018867924528301886, 'note': 0.009433962264150943, '$133': 0.009433962264150943, 'seedbed': 0.009433962264150943, 'showing': 0.009433962264150943, 'record': 0.009433962264150943, 'point': 0.009433962264150943, 'bank': 0.009433962264150943, 'dome': 0.009433962264150943, 'thin': 0.009433962264150943, 'soft': 0.009433962264150943, 'long': 0.009433962264150943, 'sketch': 0.009433962264150943, 'quick': 0.018867924528301886, 'bridge': 0.009433962264150943, 'highly': 0.009433962264150943, 'success': 0.018867924528301886, 'satisfactory': 0.009433962264150943, 'detailed': 0.009433962264150943, 'fool': 0.03773584905660377, 'paste': 0.009433962264150943, 'decisive': 0.009433962264150943, 'tidy': 0.009433962264150943, 'concrete': 0.009433962264150943, 'pleasance': 0.009433962264150943, 'fetish': 0.009433962264150943, 'wish': 0.009433962264150943, 'desert': 0.009433962264150943, 'remarkable': 0.009433962264150943, 'place': 0.009433962264150943, 'pretty': 0.009433962264150943, 'sound': 0.009433962264150943, 'hearty': 0.009433962264150943, 'Democratic': 0.009433962264150943, 'brisk': 0.009433962264150943, 'joint': 0.009433962264150943, 'compromise': 0.009433962264150943, 'wary': 0.009433962264150943, 'promise': 0.009433962264150943, 'bad': 0.009433962264150943, 'charge': 0.009433962264150943, 'complete': 0.009433962264150943, 'few': 0.018867924528301886, 'very': 0.018867924528301886, 'small': 0.009433962264150943, 'choice': 0.018867924528301886, 'man': 0.009433962264150943, 'dry': 0.009433962264150943, 'larger': 0.009433962264150943, 'preacher': 0.009433962264150943, 'report': 0.009433962264150943, 'poor': 0.009433962264150943, 'visit': 0.009433962264150943, 'home': 0.009433962264150943, 'person': 0.009433962264150943, 'woman': 0.009433962264150943, 'definite': 0.009433962264150943, 'left': 0.009433962264150943, 'better': 0.009433962264150943, 'more': 0.009433962264150943, 'drink': 0.009433962264150943, 'swift': 0.009433962264150943, 'treaty': 0.009433962264150943, 'mark': 0.009433962264150943, 'difference': 0.009433962264150943, 'break': 0.009433962264150943, 'gas': 0.009433962264150943, 'high-speed': 0.009433962264150943, 'living': 0.009433962264150943, 'cross': 0.009433962264150943, 'life': 0.009433962264150943, 'stand': 0.009433962264150943, 'list': 0.009433962264150943, 'safe': 0.009433962264150943, 'lot': 0.009433962264150943, 'settlement': 0.009433962264150943, 'hole': 0.009433962264150943, 'banker': 0.009433962264150943, 'hobby': 0.009433962264150943}\n",
            "['good', 'little', 'fool', 'start', 'right', 'quick', 'success', 'few', 'very', 'choice', 'case', 'race', 'public', 'spectacle', 'valuable', 'trip', 'judgment', 'beginning', 'note', '$133', 'seedbed', 'showing', 'record', 'point', 'bank', 'dome', 'thin', 'soft', 'long', 'sketch', 'bridge', 'highly', 'satisfactory', 'detailed', 'paste', 'decisive', 'tidy', 'concrete', 'pleasance', 'fetish', 'wish', 'desert', 'remarkable', 'place', 'pretty', 'sound', 'hearty', 'Democratic', 'brisk', 'joint', 'compromise', 'wary', 'promise', 'bad', 'charge', 'complete', 'small', 'man', 'dry', 'larger', 'preacher', 'report', 'poor', 'visit', 'home', 'person', 'woman', 'definite', 'left', 'better', 'more', 'drink', 'swift', 'treaty', 'mark', 'difference', 'break', 'gas', 'high-speed', 'living', 'cross', 'life', 'stand', 'list', 'safe', 'lot', 'settlement', 'hole', 'banker', 'hobby']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_ngrams = []\n",
        "for i in ([1,2,3,5,10]):\n",
        "    ng = ngrams(brown.words(),i)\n",
        "    list_of_ngrams.append(ng)"
      ],
      "metadata": {
        "id": "L677AQ3vgfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ngram model\n",
        "model_ngram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "print(model_ngram)\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in model_ngram:\n",
        "    for w1 in ngrams(model_ngram,1, pad_right=True, pad_left=True):\n",
        "        model_ngram[(w1, w2)][w3] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_ngram:\n",
        "    total_count = float(sum(model_ngram[w1_w2].values()))\n",
        "    for w3 in model_ngram[w1_w2]:\n",
        "        model_ngram[w1_w2][w3] /= total_count\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXZDQ_XuhAn-",
        "outputId": "da85b279-42a4-409b-92ad-28ea76689fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<function <lambda> at 0x7f5dc28b55f0>, {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram model\n",
        "\n",
        "model_bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model_bigram[(w1)][w3] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_bigram:\n",
        "    total_count = float(sum(model_bigram[w1_w2].values()))\n",
        "    for w3 in model_bigram[w1_w2]:\n",
        "        model_bigram[w1_w2][w3] /= total_count\n",
        "  \n",
        "print(dict(model_bigram[\"I\"])) "
      ],
      "metadata": {
        "id": "CxvlSgXGkgaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc960484-fb50-4745-948c-c408ae1b5ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{None: 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fivegram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3, w4,w5 in ngrams(sentence, 5, pad_right=True, pad_left=True):\n",
        "        model_fivegram[(w1,w2, w3, w4)][w5] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_fivegram:\n",
        "    total_count = float(sum(model_fivegram[w1_w2].values()))\n",
        "    for w3 in model_fivegram[w1_w2]:\n",
        "        model_fivegram[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "HyXyETsnPv7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tengram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurance  \n",
        "for sentence in brown.sents():\n",
        "    for w1, w2, w3, w4,w5, w6, w7, w8,w9, w10 in ngrams(sentence, 10, pad_right=True, pad_left=True):\n",
        "        model_tengram[(w1,w2, w3, w4,w5, w6, w7, w8,w9,)][w10] += 1\n",
        "        \n",
        "#normalizinging the frequency of co-occurence \n",
        "for w1_w2 in model_tengram:\n",
        "    total_count = float(sum(model_tengram[w1_w2].values()))\n",
        "    for w10 in model_tengram[w1_w2]:\n",
        "        model_tengram[w1_w2][w10] /= total_count"
      ],
      "metadata": {
        "id": "TYNgEwGeSKGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listOfDicts = []\n",
        "def func(sentences, model):\n",
        "    for sentence in sentences:\n",
        "        l = []\n",
        "        dict2 = {}\n",
        "        text = sentence\n",
        "        text_length_initial = len(text)\n",
        "        res = dict(sorted(model[tuple(text[-text_length_initial:])].items(), key = itemgetter(1), reverse = True)[:10])\n",
        "        lista = [x for x in res.keys()]\n",
        "        listOfDicts.append(lista) \n",
        "    return(listOfDicts)\n",
        "\n",
        "list_of_predicted_model = []\n",
        "models = [model_ngram, model_bigram, model, model_fivegram, model_tengram]\n",
        "for model in models:\n",
        "    list_of_predicted_model.append(func(listOfSentences, model))\n",
        "\n",
        "len(list_of_predicted_model[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9_Hrl1XSjh7",
        "outputId": "4741fc94-6138-472a-e1be-52849aec49f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "990"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qrel = listOfCorrectWords\n",
        "run = listOfDicts\n",
        "\n",
        "def s_at_k(correct_words, predicted_words, k=[1, 2, 3, 5, 10]):\n",
        "    count = []\n",
        "    for i in range(198):\n",
        "        words_candidate = predicted_words[i]\n",
        "        correct_word = correct_words[i]\n",
        "        for j in k:\n",
        "            if correct_word in words_candidate[:j]:\n",
        "                count.append(1)\n",
        "            else:\n",
        "                count.append(0)\n",
        "                pass\n",
        "    return(sum(count) / len(count))"
      ],
      "metadata": {
        "id": "Lo-bAtjnUjhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK1 = []\n",
        "for i in range(5):\n",
        "    meansModelsK1.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[1]))\n",
        "print(meansModelsK1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtU_jFAOU77S",
        "outputId": "339025ce-54d5-46be-ea53-953ca0c999d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK2 = []\n",
        "for i in range(5):\n",
        "    meansModelsK2.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[2]))\n",
        "print(meansModelsK2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OMJJr6lVkrl",
        "outputId": "b3e04fbf-f8fa-4f6a-c21c-f89902e504bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK3 = []\n",
        "for i in range(5):\n",
        "    meansModelsK3.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[3]))\n",
        "print(meansModelsK3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "304i8Z1HWvm7",
        "outputId": "ded81d5f-7aea-4a0b-fe22-0d5b8746176d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK5 = []\n",
        "for i in range(5):\n",
        "    meansModelsK5.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[5]))\n",
        "print(meansModelsK5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYU-PMAtXKVY",
        "outputId": "be21d83d-305b-4cbf-f036-c8bd845610a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meansModelsK10 = []\n",
        "for i in range(5):\n",
        "    meansModelsK10.append(s_at_k(listOfCorrectWords, list_of_predicted_model[i],k=[10]))\n",
        "print(meansModelsK10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-TskXNCXlJb",
        "outputId": "d069d4a0-897b-4b67-e4d4-495d41b6a3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model in list_of_ngrams:\n",
        "    qrel = listOfCorrectWords \n",
        "    run = listOfDicts\n",
        "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
        "        qrel,\n",
        "        {\n",
        "            \"success_1\",\n",
        "            \"success_2\",\n",
        "            \"success_3\",\n",
        "            \"success_4\",\n",
        "            \"success_5\",\n",
        "            \"success_6\",\n",
        "            \"success_7\",\n",
        "            \"success_8\",\n",
        "            \"success_9\",\n",
        "            \"success_10\"\n",
        "        },\n",
        "    )\n",
        "\n",
        "    res = evaluator.evaluate(run)\n",
        "    \n",
        "    # printing scores \n",
        "    successes = dict()\n",
        "    for i in range(1, 11):\n",
        "        successes[f\"success_{i}\"] = []\n",
        "\n",
        "    for inc in res:\n",
        "        tmp = res[inc]\n",
        "        for i in range(1, 11):\n",
        "            successes[f\"success_@_{i}\"].append(tmp[f\"success_@_{i}\"])\n",
        "    \n",
        "    successes = dict()\n",
        "    for i in range(1, 11):\n",
        "        successes[f\"success_@_{i}\"] = []\n",
        "\n",
        "    for inc in res:\n",
        "        tmp = res[inc]\n",
        "        for i in range(1, 11):\n",
        "            successes[f\"success_@_{i}\"].append(tmp[f\"success_@_{i}\"])\n",
        "\n",
        "    import numpy as np\n",
        "    for i in range(1, 11):\n",
        "        print(f\"average s@top-{i}: \", np.array(successes[f\"success_@_{i}\"]).mean())  \n",
        "\n",
        "# Note: Error found in pytrec_eval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "hrHLMTlxXxzW",
        "outputId": "c5515529-32d9-4684-8d8e-57736dba03fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-7a522386c353>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;34m\"success_8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;34m\"success_9\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;34m\"success_10\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         },\n\u001b[1;32m     18\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytrec_eval/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_relevance, measures, relevance_level)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mmeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_nicknames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mmeasures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_relevance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_relevance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeasures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeasures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelevance_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelevance_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Argument query_relevance should be of type dictionary."
          ]
        }
      ]
    }
  ]
}